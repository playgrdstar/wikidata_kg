{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.json_dump import WikidataJsonDump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://archive.org/details/wikimediadownloads?and[]=\"Wikidata+entity+dumps\"\n",
    "# file should be a compressed json file\n",
    "# example, we downloaded wikidata-20201130-all.json.bz2 \n",
    "date = 'wikidata-20201130-all'\n",
    "filename = f'{date}.json.bz2'\n",
    "\n",
    "# create an instance of WikidataJsonDump\n",
    "wjd_dump_path = filename\n",
    "wjd = WikidataJsonDump(str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One use of wikidata is to generate a graph of entity-entity knowledge graph relations\n",
    "# say we want to generate a graph of company-to-company relationships where the companies are listed on NYSE and NASDAQ\n",
    "nasdaq_tickers = pd.read_csv(data_dir/'NASDAQ_wiki.csv', names=['ticker', 'wikidataid'])\n",
    "nyse_tickers = pd.read_csv(data_dir/'NYSE_wiki.csv', names=['ticker', 'wikidataid'])\n",
    "nasdaq_tickers['mkt'] = 'NASDAQ'\n",
    "nyse_tickers['mkt'] = 'NYSE'\n",
    "tickers = pd.concat([nyse_tickers, nasdaq_tickers], axis=0)\n",
    "assert len(tickers) == len(nasdaq_tickers) + len(nyse_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90880584it [19:18:58, 1306.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# This runs for a while due to number of json statements\n",
    "# basically there are 'item' and 'property' types\n",
    "# we can just run through each of the statements, find 'items' and match the id based on entity we want\n",
    "# then there will be a list of claims which are the property statements\n",
    "# look through them and extract all of the relevant claims - each a key\n",
    "# each key can have multiple items linked to the claims so iterate through the list\n",
    "# then extract the relevant claims that are 'wikibase-item', cause there are other possible items related to claims like quantities\n",
    "company_list = list(tickers.wikidataid)\n",
    "company_triplets = []\n",
    "for index, entity_dict in tqdm(enumerate(wjd)):\n",
    "    if entity_dict[\"type\"] == \"item\":\n",
    "        if entity_dict[\"id\"] in company_list:\n",
    "            company = entity_dict\n",
    "            # print(company['id'], company['labels']['en']['value'])\n",
    "            try:\n",
    "                for claim in company['claims'].keys():\n",
    "                    for claimno in range(len(company['claims'][claim])):\n",
    "                        if company['claims'][claim][claimno]['mainsnak']['datatype'] == 'wikibase-item':\n",
    "                            # print(f\"{claim} | Q{company['claims'][claim][claimno]['mainsnak']['datavalue']['value']['numeric-id']}\")\n",
    "                            company_triplets.append({'source': company['id'], 'source_name': company['labels']['en']['value'], 'relationship': claim, \n",
    "                                                    'target': 'Q' + str(company['claims'][claim][claimno]['mainsnak']['datavalue']['value']['numeric-id']),\n",
    "                                                    'ticker': tickers[tickers.wikidataid == company['id']].ticker.values[0], \n",
    "                                                    'id': tickers[tickers.wikidataid == company['id']].wikidataid.values[0], \n",
    "                                                    'mkt': tickers[tickers.wikidataid == company['id']].mkt.values[0]})\n",
    "            except:\n",
    "                company_triplets.append({'source': company['id'], 'source_name': 'NIL', 'relationship': 'NIL', 'target': 'NIL',\n",
    "                                                    'ticker': tickers[tickers.wikidataid == company['id']].ticker.values[0], \n",
    "                                                    'id': tickers[tickers.wikidataid == company['id']].wikidataid.values[0], \n",
    "                                                    'mkt': tickers[tickers.wikidataid == company['id']].mkt.values[0]})\n",
    "\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_company_triplets = pd.DataFrame(company_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'full_company_relationships_triplets_' + date + '.csv'\n",
    "hdf_filename = 'full_company_relationships_triplets_keydata_' + date + '.h5'\n",
    "full_company_triplets.to_csv(data_dir/csv_filename, index=False)\n",
    "full_company_triplets.to_hdf(data_dir/hdf_filename, key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "name": "python385jvsc74a57bd0e03279686b3b8d4d34db62935c6ffaabb3e0b5b1fa0c9a858b6ef076aaf4b9ef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
